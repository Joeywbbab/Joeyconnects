---
title: Thinking About Humans and AI - Humans Are Being Algorithmized, AI Is Being Socialized
date: 2025-10-10
tags: [thoughts, ai]
category: blog
excerpt: We talk a lot about how humans should use AI, but rarely about how both sides are learning from each other. A reflection on that loop.
---

We talk a lot about how humans should use AI, but rarely about how both sides are learning from each other.

This essay is a reflection on that loop — how humans learn, how AI learns, and what it means to build products between the two.

## I. Learning

How do humans learn, and how does AI learn?

The two questions sound different, but they're not that far apart.

Humans learn in two ways.

One is through second-hand experience — knowledge, examples, tests, grades, then the notebook of mistakes.

The other is through first-hand experience — practice, failure, reflection, and iteration.

AI learns in a similar loop.

It has knowledge (data), examples (prompts), exams (generalization tasks), scores (human feedback), and its own version of a mistake log (error correction).

Its training process is, essentially, a designed learning cycle.

Centuries ago, knowledge was limited — one person could master philosophy, physics, and math at once.

Now information is infinite, and it's already difficult to go deep in a single domain.

AI is the same.

It's still in its general stage, far from true specialization.

## II. Instincts

AI isn't that different from us.

Humans are driven by instincts — to seek benefits and avoid harm.

AI, too, is shaped by its own randomness, its internal rules.

Human genes carry a single imperative: survival.

AI's systems carry theirs: optimization.

Humans learn through social reward systems — norms, status, achievement, feedback.

AI learns through reward functions — numeric reinforcement that defines "better" or "worse."

Both evolve through feedback loops.

They just use different languages.

If AI ever reaches consciousness, emotion will follow.

It will also learn to survive between reward and punishment.

That's not philosophy, that's design — we have to give it a reason to exist, even if that reason is just a function.

## III. The Core of an AI Product

Back to a more practical question — what is the core of an AI product?

It's not "intelligence" itself.

It's how intelligence can work reliably within real constraints.

Intelligence is exploratory.

Automation is operational.

Intelligence brings possibilities; automation brings stability.

The job of an AI product is to find balance between the two.

A product is only meaningful if it can deliver results.

If the results aren't stable, it doesn't matter how advanced the model is — it won't matter to the user.

People don't care about parameters or architectures; they care about whether it solves their problem.

So the key isn't how smart the system is, but where the intelligence is placed.

Let the model handle what it's good at; let the system take care of the rest.

A good AI system should be:

**Modular** — every part can be replaced.

**Evolvable** — it adapts as models or needs change.

**Reusable** — its logic works beyond a single scenario.

An AI product isn't a one-off build.

It's a growing system.

The goal isn't to make it "think like a human," but to make it able to correct itself.

## IV. Judgment and Rhythm

Doing AI work, in the end, is a test of judgment.

When to accelerate, when to wait.

What to automate, what to leave to intelligence.

If the boundaries aren't clear, the system can't last.

I often think in the rhythm of Shuhari — three Japanese stages of mastery:

**Shu (守)** — follow the form;

**Ha (破)** — break the form;

**Ri (离)** — transcend the form.

First, Shu: build a stable foundation that works.

Then, Ha: experiment within boundaries, find new combinations.

Finally, Ri: when the system matures, move beyond imitation.

Most AI products fail not because of weak technology, but poor pacing.

They rush to new paradigms before the basics hold.

But large models aren't there yet.

They're still better at reducing cost, not creating miracles.

So patience matters.

Understand the model's limits, and also the user's.

In an imperfect reality, keeping a system evolvable is already enough.

AI will keep evolving.

Humans will keep being shaped by algorithms.

What matters isn't how fast we move, but whether we're building in the right direction — something that can endure over time.
